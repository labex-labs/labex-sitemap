import requests
import xml.etree.ElementTree as ET
from datetime import datetime, UTC
from collections import defaultdict
from pathlib import Path
import json
import os
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(message)s", datefmt="%H:%M:%S"
)
logger = logging.getLogger(__name__)


def fetch_sitemap(url):
    """è·å– sitemap å†…å®¹"""
    try:
        headers = {}
        # å¦‚æœæ˜¯ labex.io çš„è¯·æ±‚ï¼Œæ·»åŠ  x-auth å¤´
        if "labex.io" in url:
            x_auth = os.getenv("LABEX_X_AUTH")
            if x_auth:
                headers["x-auth"] = x_auth
            else:
                logger.warning("æœªæ‰¾åˆ° LABEX_X_AUTH ç¯å¢ƒå˜é‡")

        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        logger.error(f"è·å–ç«™ç‚¹åœ°å›¾å¤±è´¥ï¼š{e}")
        return None


def parse_sitemap_index(xml_content):
    """è§£æ sitemap ç´¢å¼•æ–‡ä»¶"""
    root = ET.fromstring(xml_content)
    sitemaps = {}

    for sitemap in root.findall(
        ".//{http://www.sitemaps.org/schemas/sitemap/0.9}sitemap"
    ):
        loc = sitemap.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc").text
        sitemap_type = loc.split("/")[-1].replace("-sitemap.xml", "")
        sitemaps[sitemap_type] = loc

    return sitemaps


def parse_sub_sitemap(xml_content):
    """è§£æå­ sitemap æ–‡ä»¶ï¼Œè·å–æ‰€æœ‰ URL"""
    root = ET.fromstring(xml_content)
    urls = []

    for url in root.findall(".//{http://www.sitemaps.org/schemas/sitemap/0.9}url"):
        loc = url.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc").text
        lastmod_elem = url.find("{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod")
        lastmod = lastmod_elem.text if lastmod_elem is not None else None

        urls.append({"loc": loc, "lastmod": lastmod})

    return urls


def get_repository_structure():
    """è·å–ä»“åº“çš„å®é™…ç›®å½•ç»“æ„"""

    def create_tree(path, prefix="", is_last=True):
        """é€’å½’åˆ›å»ºç›®å½•æ ‘"""
        output = []
        path_obj = Path(path)

        # å¿½ç•¥çš„ç›®å½•å’Œæ–‡ä»¶
        ignore = {".git", ".github", "__pycache__", ".gitignore", ".DS_Store"}

        # è·å–ç›®å½•å†…å®¹å¹¶æ’åº
        items = sorted([x for x in path_obj.iterdir() if x.name not in ignore])

        for i, item in enumerate(items):
            is_last_item = i == len(items) - 1

            # ç¡®å®šæ˜¾ç¤ºçš„å‰ç¼€
            current_prefix = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
            child_prefix = "    " if is_last_item else "â”‚   "

            # æ·»åŠ å½“å‰é¡¹
            output.append(f"{prefix}{current_prefix}{item.name}")

            # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’å¤„ç†
            if item.is_dir():
                output.extend(
                    create_tree(
                        item, prefix=prefix + child_prefix, is_last=is_last_item
                    )
                )

        return output

    # ä»å½“å‰ç›®å½•å¼€å§‹ç”Ÿæˆæ ‘
    tree_lines = ["```", "labex-sitemap/"] + create_tree(".")
    tree_lines.append("```")

    return "\n".join(tree_lines)


def generate_category_markdown(category, data):
    """ç”Ÿæˆæ¯ä¸ªç±»åˆ«çš„ markdown å†…å®¹"""
    markdown = f"""---
layout: default
---

# {category.title()} Sitemap

> Last updated: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M UTC')}

This file contains all {category.lower()} related links from LabEx website.

## Sitemap

[{data['sitemap_url'].split('/')[-1]}]({data['sitemap_url']})

## Links

"""
    if data["urls"]:
        # å°† URL æŒ‰ç…§è·¯å¾„æ·±åº¦å’Œå­—æ¯é¡ºåºæ’åº
        sorted_urls = sorted(
            data["urls"], key=lambda x: (len(x["loc"].split("/")), x["loc"])
        )

        # åˆ›å»ºä¸€ä¸ªåŸºäºè·¯å¾„çš„æ ‘çŠ¶ç»“æ„
        url_tree = defaultdict(list)
        for url_data in sorted_urls:
            url = url_data["loc"]
            path_parts = url.replace("https://labex.io/", "").split("/")
            if len(path_parts) > 1:
                key = path_parts[0]
                url_tree[key].append(url_data)
            else:
                url_tree["root"].append(url_data)

        # æŒ‰ç…§åˆ†ç»„ç”Ÿæˆ markdown
        for group, urls in sorted(url_tree.items()):
            if group != "root":
                markdown += f"\n### {group}\n\n"
            for url_data in urls:
                url = url_data["loc"]
                lastmod = url_data["lastmod"]
                display_name = (
                    url.split("/")[-1] if url.split("/")[-1] else url.split("/")[-2]
                )
                markdown += f"- [{display_name}]({url})"
                if lastmod:
                    markdown += f" *(Last modified: {lastmod})*"
                markdown += "\n"

    return markdown


def generate_main_readme(sitemaps_with_urls):
    """ç”Ÿæˆä¸» README æ–‡ä»¶çš„å†…å®¹"""
    markdown = f"""---
layout: default
---

# LabEx Sitemap

[LabEx](https://labex.io) is a hands-on learning platform for Linux, DevOps, and Cybersecurity. Learn by doing with guided labs, courses, and tutorials. Get started for free!

> Last updated: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M UTC')}

This repository maintains an auto-updated list of LabEx website sitemaps.

## Categories

"""
    # æ·»åŠ åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯
    total_links = 0
    for category, data in sorted(sitemaps_with_urls.items()):
        num_links = len(data["urls"])
        total_links += num_links
        markdown += f"- [{category.title()}](categories/{category.lower()}.md) ({num_links} links)\n"

    markdown += f"\n> **Total Links: {total_links}**\n"

    return markdown


def ensure_directory_exists(directory):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(directory).mkdir(parents=True, exist_ok=True)


def update_files(sitemaps_with_urls):
    """æ›´æ–°æ‰€æœ‰ markdown æ–‡ä»¶"""
    # ç¡®ä¿ sitemaps ç›®å½•å­˜åœ¨
    ensure_directory_exists("categories")

    # æ›´æ–°ä¸» README
    main_content = generate_main_readme(sitemaps_with_urls)
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(main_content)

    # æ›´æ–°æ¯ä¸ªç±»åˆ«çš„æ–‡ä»¶
    for category, data in sitemaps_with_urls.items():
        category_content = generate_category_markdown(category, data)
        filename = f"categories/{category.lower()}.md"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(category_content)


def save_link_counts(sitemaps_with_urls):
    """Save current link counts to a file for comparison"""
    counts = {}
    for category, data in sitemaps_with_urls.items():
        counts[category] = len(data["urls"])

    counts_file = "link_counts.json"
    with open(counts_file, "w", encoding="utf-8") as f:
        json.dump(counts, f, indent=2)

    return counts


def load_previous_link_counts():
    """Load previous link counts from file"""
    counts_file = "link_counts.json"
    try:
        if os.path.exists(counts_file):
            with open(counts_file, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        logger.warning(f"åŠ è½½å†å²é“¾æ¥æ•°æ®å¤±è´¥ï¼š{e}")

    return {}


def send_feishu_notification(title, text):
    """Send notification to Feishu webhook"""
    webhook_url = os.getenv("FEISHU_WEBHOOK_URL")
    if not webhook_url:
        logger.warning("æœªæ‰¾åˆ°é£ä¹¦ Webhook åœ°å€")
        return False

    try:
        response = requests.post(
            webhook_url,
            json={"title": title, "text": text, "to": "huhuhang"},
        )
        response.raise_for_status()
        logger.info("é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ")
        return True
    except requests.RequestException as e:
        logger.error(f"é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥ï¼š{e}")
        return False


def check_and_notify_link_changes(current_counts, previous_counts):
    """Check for significant link count changes and send notifications"""
    if not previous_counts:
        logger.info("é¦–æ¬¡è¿è¡Œï¼Œè·³è¿‡å˜åŒ–æ£€æµ‹")
        return

    total_current = sum(current_counts.values())
    total_previous = sum(previous_counts.values())
    total_change = total_current - total_previous

    # Check total change only
    if abs(total_change) > 100:
        # Prepare notification message
        title = "LabEx ç½‘ç«™åœ°å›¾é“¾æ¥æ•°é‡å˜åŒ–æé†’"

        text_parts = [
            f"ğŸ”” **ç½‘ç«™åœ°å›¾é“¾æ¥å˜åŒ–æ£€æµ‹**",
            f"",
            f"ğŸ“Š **æ€»é“¾æ¥æ•°ç»Ÿè®¡ï¼š**",
            f"â€¢ ä¹‹å‰æ•°é‡ï¼š{total_previous:,}",
            f"â€¢ å½“å‰æ•°é‡ï¼š{total_current:,}",
            f"â€¢ å˜åŒ–æ•°é‡ï¼š{total_change:+,}",
            f"",
            f"ğŸ• **æ›´æ–°æ—¶é—´ï¼š** {datetime.now(UTC).strftime('%Y-%m-%d %H:%M UTC')}",
            f"ğŸ”— **ä»£ç ä»“åº“ï¼š** https://github.com/labex-labs/labex-sitemap",
        ]

        text = "\n".join(text_parts)

        logger.info(f"æ£€æµ‹åˆ°é‡å¤§å˜åŒ–ï¼š{total_change:+,} ä¸ªé“¾æ¥")
        send_feishu_notification(title, text)
    else:
        logger.info(f"é“¾æ¥å˜åŒ–åœ¨é˜ˆå€¼èŒƒå›´å†…ï¼š{total_change:+,}")


def update_package_version():
    """Update package.json version number"""
    try:
        # Read the current package.json
        with open("package.json", "r") as f:
            package_data = json.load(f)

        # Split version into parts
        major, minor, patch = map(int, package_data["version"].split("."))

        # Increment patch version
        patch += 1

        # Update version in package data
        package_data["version"] = f"{major}.{minor}.{patch}"

        # Write back to package.json with proper formatting
        with open("package.json", "w") as f:
            json.dump(package_data, f, indent=2)
            # Add newline at end of file
            f.write("\n")

        logger.info(f"ç‰ˆæœ¬æ›´æ–°è‡³ {package_data['version']}")

    except Exception as e:
        logger.error(f"ç‰ˆæœ¬æ›´æ–°å¤±è´¥ï¼š{e}")


def generate_llms_txt(sitemaps_with_urls):
    """Generate a llms.txt file with all categories and links"""
    content = ""

    # Process each category
    for category, data in sorted(sitemaps_with_urls.items()):
        # Add category header
        content += f"# {category.title()}\n\n"

        # Create a tree structure similar to what's done in generate_category_markdown
        url_tree = defaultdict(list)
        sorted_urls = sorted(
            data["urls"], key=lambda x: (len(x["loc"].split("/")), x["loc"])
        )

        for url_data in sorted_urls:
            url = url_data["loc"]
            path_parts = url.replace("https://labex.io/", "").split("/")
            if len(path_parts) > 1:
                key = path_parts[0]
                url_tree[key].append(url_data)
            else:
                url_tree["root"].append(url_data)

        # Process each group in the category
        for group, urls in sorted(url_tree.items()):
            if group != "root":
                content += f"## {group}\n\n"
            else:
                content += "## \n\n"  # Empty section header for root items

            # Add links
            for url_data in urls:
                url = url_data["loc"]
                display_name = (
                    url.split("/")[-1] if url.split("/")[-1] else url.split("/")[-2]
                )
                content += f"- [{display_name}]({url})\n"

            content += "\n"

    # Write content to file
    with open("llms.txt", "w", encoding="utf-8") as f:
        f.write(content)

    logger.info("ç”Ÿæˆ llms.txt æ–‡ä»¶")


def main():
    logger.info("å¼€å§‹æ›´æ–°ç«™ç‚¹åœ°å›¾")

    # Load previous link counts for comparison
    previous_counts = load_previous_link_counts()

    # è·å– sitemap ç´¢å¼•
    sitemap_index_url = "https://labex.io/sitemap_index.xml"
    logger.info("è·å–ç«™ç‚¹åœ°å›¾ç´¢å¼•")
    xml_content = fetch_sitemap(sitemap_index_url)

    if xml_content:
        # è§£æä¸» sitemap
        sitemaps = parse_sitemap_index(xml_content)
        logger.info(f"å‘ç° {len(sitemaps)} ä¸ªå­ç«™ç‚¹åœ°å›¾")

        # å­˜å‚¨æ‰€æœ‰ sitemap åŠå…¶åŒ…å«çš„ URL
        sitemaps_with_urls = {}
        total_urls = 0

        # è·å–æ¯ä¸ªå­ sitemap çš„å†…å®¹
        for sitemap_type, sitemap_url in sitemaps.items():
            logger.info(f"å¤„ç† {sitemap_type} ç«™ç‚¹åœ°å›¾")
            sub_sitemap_content = fetch_sitemap(sitemap_url)

            if sub_sitemap_content:
                urls = parse_sub_sitemap(sub_sitemap_content)
                sitemaps_with_urls[sitemap_type] = {
                    "sitemap_url": sitemap_url,
                    "urls": urls,
                }
                total_urls += len(urls)
                logger.info(f"{sitemap_type}: {len(urls)} ä¸ªé“¾æ¥")
            else:
                logger.error(f"{sitemap_type} ç«™ç‚¹åœ°å›¾è·å–å¤±è´¥")
                sitemaps_with_urls[sitemap_type] = {
                    "sitemap_url": sitemap_url,
                    "urls": [],
                }

        logger.info(f"æ€»è®¡ {total_urls} ä¸ªé“¾æ¥")

        # Save current link counts and check for significant changes
        current_counts = save_link_counts(sitemaps_with_urls)
        check_and_notify_link_changes(current_counts, previous_counts)

        # æ›´æ–°æ‰€æœ‰æ–‡ä»¶
        logger.info("æ›´æ–° Markdown æ–‡ä»¶")
        update_files(sitemaps_with_urls)

        # Generate llms.txt file
        generate_llms_txt(sitemaps_with_urls)

        # Update package version
        update_package_version()

        logger.info("æ‰€æœ‰æ–‡ä»¶æ›´æ–°å®Œæˆ")
    else:
        logger.error("ç«™ç‚¹åœ°å›¾ç´¢å¼•è·å–å¤±è´¥")


if __name__ == "__main__":
    main()
